Q: Kaggleのマウスの行動認識(https://www.kaggle.com/competitions/MABe-mouse-behavior-detection/overview)をやっていて動画から抽出され部位の座標データからマウスの行動を認識するんだが、膨大な座標データをどう取り扱えばいいのか試行錯誤している。

A: なるほど 👍 Kaggle の **MABe (Mouse Behavior Detection)** は「動画から抽出されたマウスの身体部位の座標」→「行動クラス分類」という典型的な **時系列行動認識** の問題ですね。膨大な座標データをどう扱うかはモデル精度と計算効率の両立がポイントです。整理すると大きく以下の戦略があります：

---

## 1. 座標データの前処理

座標はそのままでは冗長なので、まず特徴量設計が重要です。

* **正規化**

  * 画像座標を [0,1] にスケーリング
  * 体の中心点（例: 胴体中心）を基準に平行移動
  * マウスの向きに合わせて回転補正（向き不変な特徴にする）

* **座標差分**

  * 各フレーム間の速度・加速度（位置差分・二階差分）
  * 特定部位間の距離（鼻 ↔ 尾、前足 ↔ 後足 など）
  * 角度（鼻・首・尾を結んだ角度など）

* **次元削減**

  * PCA / UMAP で主要な運動パターンに圧縮
  * スケルトンベース特徴量を抽出（OpenPose 研究の文脈と似ています）

---

## 2. 特徴量から行動分類へのアプローチ

### (a) 統計特徴 + 従来ML

* 一定時間窓（例: 1秒=30フレーム）で統計量を計算
  （平均位置、移動距離、速度の分散、角度の変化など）
* それを LightGBM / XGBoost / RandomForest で分類

長所: 軽い・解釈性あり
短所: 時系列依存が弱くなる

---

### (b) 時系列モデル

* **RNN/LSTM/GRU**

  * 座標列をそのまま入力

* **1D CNN**

  * フレームごとの特徴を時系列畳み込みで処理

* **Transformer**

  * Self-Attention で長期依存を扱える（最近のトレンド）

---

### (c) 動作パターンをクラスタリングしてから分類

* 行動の「基本的な運動モチーフ」を unsupervised にクラスタリング
* クラスタ系列を HMM や RNN で分類

---

## 3. 実際の流れイメージ

1. **前処理**

   ```python
   # 例: pandasで座標を読み込み
   coords = pd.read_csv("mouse_coords.csv")
   # 正規化
   coords["x"] /= frame_width
   coords["y"] /= frame_height
   # 速度
   coords["vx"] = coords["x"].diff()
   coords["vy"] = coords["y"].diff()
   ```
2. **特徴量設計**

   * 距離: `np.linalg.norm([x1-x2, y1-y2])`
   * 角度: `np.arctan2(y2-y1, x2-x1)`

3. **スライディングウィンドウ**

   * 例: 30フレーム単位で統計量を計算

4. **分類器**

   * LightGBM でベースライン
   * 精度が必要なら Transformer 系に移行

---

## 4. ベースライン作りのおすすめ順

1. **窓統計量 + LightGBM** でまず動くモデルを作る
2. その後 **LSTM / 1D CNN** で時系列依存をモデリング
3. 最後に **Transformer (TimeSformer, ViT for skeletons)** を試す

---

💡質問ですが、あなたが今悩んでいるのは

* **(A)** 「座標をどう前処理・特徴量化すればいいのか」なのか
* **(B)** 「モデルにどう入れればいいのか（LSTM？LightGBM？）」なのか
* **(C)** 「計算資源的にどう処理を効率化すればいいか」なのか

どの方向に深掘りしましょうか？





Q: (A)かな。
train.csvとtrain_annotationから既に抽出された行動は得ることはできる。
行動時間が短時間でベクトル化すれば何とかなると思っている。
しかし、trackingの座標データが長時間なので、そこから1秒や2秒単位でベクトル化すればいいのかが悩む。

A: いいですね、方向性ハッキリしました — ウィンドウ長（1秒/2秒etc.）の選び方と実践的な手順を**すぐ試せる形で**まとめます。長めに書きますが、最後に「すぐ試すための最短コード」もつけます。🍣

# 要点（結論）

* **正解は「データ依存」**。行動の実際の継続時間分布を見てから決めるのが最も確実。
* 実務的な初期候補としては **0.25s, 0.5s, 1.0s, 2.0s, 4.0s** を試し、**評価で最も良いものを選ぶ**（下で具体的な実験設計を提示）。
* **オーバーラップ（窓のstride）を入れる**：50%〜75% が実用的（イベントが窓境界で分断されるのを防ぐため）。
* 短い行動は小さいウィンドウ（あるいはラベル付けルール）で扱い、長い行動は大きめで。**マルチスケール（複数ウィンドウ）を組み合わせる**と性能向上しやすい。

---

# なぜ「データ依存」か（判断基準）

1. **行動の持続時間**：多くの行動が平均0.2sなら1sは長すぎて他の行動が混ざる。逆に多くが1〜3sなら1sは妥当。
2. **サンプリングレート（FPS）**：FPSが低ければ（例 10fps）短いウィンドウは情報不足。FPSが高ければ（30〜100fps）短ウィンドウが可能。
3. **クラスの区別に必要な「時間スケール」**：姿勢の瞬間差で判別できるのか、ある程度の時間的パターン（周期）が必要か。
4. **計算コスト**：短いウィンドウ＋高オーバーラップはデータ量が増える。

→ まずは **実データの行動持続時間分布（ヒストグラム／CDF）を作る**こと。中央値・75パーセンタイル・90パーセンタイルを見て判断。

---

# 実験設計（推奨）

1. **行動持続時間の解析**（必須）

   * 各 `train_annotation` の（end_time - start_time） を算出して分布を見る（中央値・90%点をメモ）。
   * 目安：90% の行動が `<= W` 秒なら W を一つの上限候補にする。

2. **ウィンドウ候補**

   * 候補リスト： `[0.25, 0.5, 1.0, 2.0, 4.0]` 秒（FPSに応じてフレーム数に変換）
   * ストライド（重なり）： `stride = window * 0.5`（50%）をまず試す。重要イベントが非常に短ければ75%も検討。

3. **ラベル付けルール（窓→1ラベル）**

   * **Majority rule（多数決）**：窓内のフレームで最も多いラベルを採用。簡単で汎用的。
   * **Threshold（閾値）**：あるクラスが窓内のフレームで > x%（例 50%）占めるならそのラベル、占めない場合は `no_action` とする。
   * **Event-centered**：アノテーションの中心（start+end/2）が窓内に入るラベルを採用（短いイベントに有利）。
   * **マルチラベル**：窓に複数アクションが混ざるなら multi-hot にして multi-label 学習も可（但し評価指標注意）。

4. **特徴量設計（窓単位）**

   * 基本統計量（各 keypoint の mean, std, min, max）
   * 運動量：総移動距離、平均速度、速度std、加速度の統計
   * 部位間距離（鼻–尾など）の mean/std, 最大変化量
   * 角度（関節角）mean/std、角速度
   * 周波数特徴（短時間Fourier / spectrogram）の主要係数（もし周期性が重要なら）
   * PCAで次元圧縮してからモデルに入れても良い

5. **モデルと評価**

   * 最初：**窓統計量 + LightGBM** で迅速に比較（軽い・早い）
   * その後：**1D-CNN / LSTM / Transformer**（窓のフレーム列をそのまま入力）で改善検証
   * 評価は **動画単位の分割（同じ動画がtrainとvalにまたがらない）**、マクロF1や各クラスF1を確認。短時間イベントの検出は「イベント検出指標（IoUやイベント毎のprecision/recall）」も重要。

---

# 短い行動への対処

* 窓が長すぎると短い行動が埋もれる → **短窓を追加してマルチスケール**にする（例：1秒窓と0.25秒窓を両方作り特徴を結合）。
* ラベルの扱い：短イベントに対して majority ルールは不利 → **イベントの中心が窓に入ったらそのラベル**のルールを併用すると改善することが多い。
* データ増強：短イベントはオーバーサンプリング、合成、窓位置シフトで補う。

---

# 計算効率の実践的ヒント

* **特徴量を一度作ってファイル保存（parquet/hdf5）**し、モデル学習はそこから読み込む。毎回再計算しない。
* フレーム単位の大規模処理は **バッチ/並列（multiprocessing）** で。pandasよりnumpyベースのループが速い場合あり。
* 大きいデータは **memory-mapped numpy (.npy, .npz)** や **pyarrow parquet** を使う。
* インクリメンタル統計（分割して平均・分散更新）でメモリ削減。

---

# すぐ試せる実装：窓化・特徴量抽出・ラベル付け（Python擬似コード）

以下はフレーム列（keypoints per frame）をウィンドウ化して統計量を作り、窓に多数派ラベルを付ける簡易例。**FPS は実データに合わせて設定**してください。

```python
import numpy as np
import pandas as pd
from scipy.spatial.distance import cdist

# --- 前提データ ---
# coords: shape (N_frames, N_keypoints*2)  (x1,y1,x2,y2,...)
# frame_labels: shape (N_frames,)  (各フレームのラベル、アノテーションから作成済み)
# fps = 30

def window_features(coords, frame_labels, fps, window_s=1.0, stride_s=None):
    if stride_s is None:
        stride_s = window_s * 0.5
    win_frames = int(round(window_s * fps))
    stride_frames = int(round(stride_s * fps))
    n_frames, dim = coords.shape
    features = []
    labels = []
    for start in range(0, n_frames - win_frames + 1, stride_frames):
        end = start + win_frames
        win = coords[start:end]  # (win_frames, dim)
        # 基本統計
        mean = win.mean(axis=0)
        std = win.std(axis=0)
        mn = win.min(axis=0)
        mx = win.max(axis=0)
        # 総移動距離（各keypoint）
        reshaped = win.reshape(win_frames, -1, 2)  # (T, K, 2)
        diffs = np.linalg.norm(np.diff(reshaped, axis=0), axis=2)  # (T-1, K)
        dist_sum = diffs.sum(axis=0)  # (K,)
        # 部位間距離の統計（例: keypoint0 と keypoint1）
        k0 = reshaped[:,0,:]  # (T,2)
        k1 = reshaped[:,1,:]
        d01 = np.linalg.norm(k0 - k1, axis=1)
        d01_mean = d01.mean(); d01_std = d01.std()
        # 特徴ベクトル結合
        feat = np.concatenate([mean, std, mn, mx, dist_sum, [d01_mean, d01_std]])
        features.append(feat)
        # ラベル: majority
        win_lbls = frame_labels[start:end]
        # if no label (e.g., background), handle accordingly
        vals, counts = np.unique(win_lbls, return_counts=True)
        lbl = vals[np.argmax(counts)]
        labels.append(lbl)
    X = np.vstack(features)
    y = np.array(labels)
    return X, y

# 使い方:
# X,y = window_features(coords, frame_labels, fps=30, window_s=1.0, stride_s=0.5)
```

この後、`X` を LightGBM で学習すれば素早く比較できます。

---

# 追加でやると良い小技（短い）

* **ウィンドウごとにフレームの相対時間（0〜1）を付ける**とモデルが時間位置を利用できる。
* **窓の両端に "context"（前後0.25s）を付加**して判別を安定化。
* **マルチヘッド出力**：短時間検出用と長時間分類用の二系統を学習してマージ。
* **バランスの良い評価**：クラス不均衡が酷いなら F1マクロやクラス別PRを確認。

---

# すぐ試すための実験プラン（優先度順）

1. 行動持続時間分布の確認 → 90%点をメモ。
2. 5つのウィンドウ候補で `window_features` を作成（stride=50%）して LightGBM を比較。
3. 最良ウィンドウに対してオーバーラップ率（25%/50%/75%）を試す。
4. 短い行動を改善するため `event-centered` ラベリングや小窓(0.25s)を追加し比較。
5. 最後に LSTM/1D-CNN にそのままフレーム列（正規化済）を入れて比較。

---

必要なら、あなたの **fps** と **train_annotation の形式（一例: start_frame/end_frame か start_time/end_time）** を使って、

* （A）行動持続時間分布を出すコード、
* （B）候補ウィンドウで一括で特徴量を作って比較する実験スクリプト（LightGBMでのクロスバリデーション付き）
  をそのまま書いて渡します。どちらを先に出しましょう？（すぐ出します — 遅延作業はしません👍）





Q: Bかな

A: 了解です 👍
では **「複数ウィンドウ長で特徴量を一括生成 → LightGBM でクロスバリデーション評価」** までの実験スクリプトを書きます。
これで「どのウィンドウ長が有効か」をすぐ比較できます。

以下は **擬似データではなく train.csv / train_annotation.csv を読む前提**の形で書いておきます。
（実データのカラム名は確認して調整してください。`frame`, `x1,y1,...`, `behavior`, などを想定）

---

## スクリプト例

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupKFold, cross_val_score
from sklearn.metrics import f1_score, make_scorer
import lightgbm as lgb

# ========== データ読み込み ==========
# 例: train.csv に座標、train_annotation にラベル (start_frame, end_frame, behavior)
coords = pd.read_csv("train.csv")  # 各フレームの座標: frame, x1,y1,x2,y2,...
anno = pd.read_csv("train_annotation.csv")

fps = 30  # 映像のフレームレート。データ仕様に合わせて調整

# ========== フレーム単位のラベル列を作成 ==========
n_frames = coords["frame"].max() + 1
frame_labels = np.array(["none"] * n_frames)

for _, row in anno.iterrows():
    start = int(row["start_frame"])
    end = int(row["end_frame"])
    frame_labels[start:end+1] = row["behavior"]

# 特徴量に使う座標部分 (frame列除く)
coord_array = coords.drop(columns=["frame"]).values  # shape = (N_frames, N_keypoints*2)


# ========== ウィンドウ特徴量抽出関数 ==========
def window_features(coords, labels, fps, window_s=1.0, stride_s=None):
    if stride_s is None:
        stride_s = window_s * 0.5
    win_frames = int(round(window_s * fps))
    stride_frames = int(round(stride_s * fps))
    n_frames, dim = coords.shape

    features, lbls = [], []
    for start in range(0, n_frames - win_frames + 1, stride_frames):
        end = start + win_frames
        win = coords[start:end]
        # 基本統計量
        mean = win.mean(axis=0)
        std = win.std(axis=0)
        mn = win.min(axis=0)
        mx = win.max(axis=0)
        # 移動距離（全keypoint）
        reshaped = win.reshape(win_frames, -1, 2)  # (T, K, 2)
        diffs = np.linalg.norm(np.diff(reshaped, axis=0), axis=2)  # (T-1,K)
        dist_sum = diffs.sum(axis=0)
        # 例: keypoint0とkeypoint1の距離統計
        k0 = reshaped[:,0,:]; k1 = reshaped[:,1,:]
        d01 = np.linalg.norm(k0 - k1, axis=1)
        d01_mean, d01_std = d01.mean(), d01.std()
        feat = np.concatenate([mean, std, mn, mx, dist_sum, [d01_mean, d01_std]])
        features.append(feat)
        # 窓のラベル（majority rule）
        vals, counts = np.unique(labels[start:end], return_counts=True)
        lbl = vals[np.argmax(counts)]
        lbls.append(lbl)

    return np.vstack(features), np.array(lbls)


# ========== 複数ウィンドウで比較 ==========
window_list = [0.25, 0.5, 1.0, 2.0, 4.0]

for win in window_list:
    X, y = window_features(coord_array, frame_labels, fps, window_s=win, stride_s=win*0.5)

    # ラベルが "none" の窓は除外（必要に応じて残しても良い）
    mask = y != "none"
    X, y = X[mask], y[mask]

    # LightGBM
    clf = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, num_leaves=31)

    # GroupKFold: 動画ごとに分割する場合は group に video_id を使う
    # 今回はダミーで全て同じグループ扱い
    groups = np.arange(len(y))

    cv = GroupKFold(n_splits=5)
    scores = cross_val_score(clf, X, y, cv=cv, groups=groups,
                             scoring=make_scorer(f1_score, average="macro"))

    print(f"Window={win:.2f}s | F1_macro={scores.mean():.4f} ± {scores.std():.4f}")
```

---

## ポイント

1. **ラベル作成**

   * `train_annotation.csv` の start/end frame をフレームごとに展開して `frame_labels` を作る。
   * 「短い行動が窓に混ざる」問題を避けるなら `majority` 以外のルール（中心イベント方式など）も試せます。

2. **特徴量**

   * 平均/分散/最大最小 + 部位間距離の統計を例示。
   * 他の距離や角度も増やしてOK。

3. **クロスバリデーション**

   * `GroupKFold` を使えば「同じ動画のデータが train/test にまたがらない」ようにできます。
   * 実際は `groups = video_id` にするのがベストです。

4. **評価指標**

   * マクロ F1 を基準に比較。
   * 最初は「どのウィンドウ長が一番効くか」だけを見れば十分。

---

👉 これを走らせれば **「1秒が良いのか0.5秒が良いのか」** がすぐ数値で見えます。
その後、最良ウィンドウで LSTM / Transformer に進めばOKです。

---

質問ですが：
あなたの `train_annotation.csv` のフォーマットって **start/end が frame 単位**ですか？ それとも **秒単位**ですか？





Q: labによってカメラが違っている模様でtrain.csvにframes_per_secという列があり、フレーム単位だと誤認識しそうな予感がするので、秒単位で合わせるようにしている

A: なるほど 👍
MABe のデータは **ラボごとにカメラ仕様が違う**ので、`train.csv` にある `frames_per_sec` を使って **時間軸を秒に正規化**するのは正解です。

フレーム番号だけを使うと「fps=15 の動画の1秒」と「fps=30 の動画の1秒」で2倍ズレるので、行動の持続時間が正しく揃いません。

---

## 秒ベースで処理する方法

1. **annotation の start/end を秒に変換**

   ```python
   anno["start_sec"] = anno["start_frame"] / anno["frames_per_sec"]
   anno["end_sec"]   = anno["end_frame"] / anno["frames_per_sec"]
   ```

   こうして「動画内の絶対秒」で表現する。

2. **coords も秒列を追加**

   ```python
   coords["time_sec"] = coords["frame"] / coords["frames_per_sec"]
   ```

   → これで座標も「秒」に揃えられる。

3. **window化は秒基準**

   * 例えば「1秒窓・0.5秒stride」なら、
     各動画ごとに `time_sec` を基準に切っていけば fps に依存しない。

---

## 具体的に書き直すと

```python
def window_features_seconds(coords, labels, fps, window_s=1.0, stride_s=None):
    if stride_s is None:
        stride_s = window_s * 0.5
    
    # coords: (N_frames, D), labels: (N_frames,)
    times = np.arange(len(coords)) / fps   # 各フレームの時間[s]
    n_frames, dim = coords.shape
    
    features, lbls = [], []
    start_t = 0.0
    while start_t + window_s <= times[-1]:
        end_t = start_t + window_s
        mask = (times >= start_t) & (times < end_t)
        if not mask.any():
            start_t += stride_s
            continue
        win = coords[mask]
        win_lbls = labels[mask]
        
        # 基本統計
        mean, std = win.mean(axis=0), win.std(axis=0)
        mn, mx = win.min(axis=0), win.max(axis=0)
        
        # 移動距離
        reshaped = win.reshape(len(win), -1, 2)
        diffs = np.linalg.norm(np.diff(reshaped, axis=0), axis=2)
        dist_sum = diffs.sum(axis=0)
        
        feat = np.concatenate([mean, std, mn, mx, dist_sum])
        features.append(feat)
        
        # 窓のラベル（majority）
        vals, counts = np.unique(win_lbls, return_counts=True)
        lbls.append(vals[np.argmax(counts)])
        
        start_t += stride_s
    
    return np.vstack(features), np.array(lbls)
```

ここで `fps` は各動画の `frames_per_sec` を使います。
動画ごとにループして `X, y` を積み上げればOK。

---

## 注意点

* **動画ごとに fps が違う**ので、必ず「1動画ごとに window_features_seconds を呼ぶ」→ その結果を結合。
* `GroupKFold` の group は `video_id` を使うと良い。
* annotation が秒単位でない場合は必ず `frames_per_sec` を掛け算/割り算して整合させる。

---

👉 これで「秒単位でウィンドウを切る」仕組みができます。
次のステップとしては、実際に `train_annotation.csv` に入っている **start/end がフレーム単位か秒単位か**を確認するのが大事ですね。

質問ですが：
あなたの `train_annotation.csv` の `start_time` / `end_time` は **frame番号**ですか？それとも **秒**で入っていますか？





Q: frame番号

A: なるほど、ありがとうございます。ではやはり **動画ごとの `frames_per_sec` を使ってフレーム → 秒に変換** してウィンドウ化するのが正解です。

ポイントを整理します：

---

## 1. フレーム → 秒変換

```python
# train_annotation に frames_per_sec 列がある場合
anno["start_sec"] = anno["start_frame"] / anno["frames_per_sec"]
anno["end_sec"]   = anno["end_frame"]   / anno["frames_per_sec"]

# coords にも秒列を追加
coords["time_sec"] = coords["frame"] / coords["frames_per_sec"]
```

* こうすることで、ラボごとに FPS が違っても **時間軸が統一**されます。
* ウィンドウ長も秒単位で指定でき、すべての動画に対して同じ「1秒窓」「0.5秒stride」などを適用可能。

---

## 2. ウィンドウ化の方針（秒単位）

* `start_sec` ～ `end_sec` をもとにフレームラベルを作る
* `time_sec` でウィンドウをスライド
* 窓のラベルは **majority rule** または **イベント中心方式** で決定
* 窓のサイズは行動持続時間に依存 → 0.25〜2秒あたりで検証

---

## 3. 実装イメージ

```python
# 1動画ごとに処理
fps = coords["frames_per_sec"].iloc[0]  # この動画の fps
coord_array = coords.drop(columns=["frame","frames_per_sec"]).values
frame_labels = np.array(["none"]*len(coords))

# フレームラベル作成
for _, row in anno.iterrows():
    start_frame = int(row["start_frame"])
    end_frame   = int(row["end_frame"])
    frame_labels[start_frame:end_frame+1] = row["behavior"]

# ウィンドウ特徴量抽出（秒単位）
X, y = window_features_seconds(coord_array, frame_labels, fps,
                               window_s=1.0, stride_s=0.5)
```

* こうすれば **FPSが違う動画でも同じ秒数のウィンドウ長で比較可能**。
* クロスバリデーションのときは `GroupKFold` で動画単位に分ければ FPSの違いの影響を吸収できます。

---

💡ポイント

* ウィンドウ長は **秒単位**で選ぶ
* FPS依存のフレーム数ではなく、秒数で切ることでラボ間差を吸収
* 速度や移動距離などの特徴も **秒単位で正規化**（フレーム差ではなく時間差）するとさらに安定

---

もし希望なら、私は **この秒単位ウィンドウでの LightGBM クロスバリデーション用スクリプト** に対応させた完全版を書けます。
作っておきますか？





Q: お願い

A: 了解です。では **秒単位ウィンドウ化 + FPS補正 + LightGBM クロスバリデーション** の完全版スクリプトを作ります。
これを使えば「複数ウィンドウ長を秒単位で比較」して、ラボ間 FPSの違いも吸収できます。

---

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupKFold, cross_val_score
from sklearn.metrics import f1_score, make_scorer
import lightgbm as lgb

# ===============================
# 1. データ読み込み
# ===============================
coords = pd.read_csv("train.csv")           # 各フレームの座標: frame, frames_per_sec, x1,y1,x2,y2,...
anno   = pd.read_csv("train_annotation.csv") # start_frame, end_frame, behavior, video_id

# ===============================
# 2. 秒単位に変換
# ===============================
anno["start_sec"] = anno["start_frame"] / anno["frames_per_sec"]
anno["end_sec"]   = anno["end_frame"]   / anno["frames_per_sec"]
coords["time_sec"] = coords["frame"] / coords["frames_per_sec"]

# ===============================
# 3. 秒単位ウィンドウ特徴量抽出関数
# ===============================
def window_features_seconds(coord_array, frame_labels, fps, window_s=1.0, stride_s=None):
    if stride_s is None:
        stride_s = window_s * 0.5

    times = np.arange(len(coord_array)) / fps  # 各フレームの時間[s]
    n_frames, dim = coord_array.shape

    features, lbls = [], []
    start_t = 0.0
    while start_t + window_s <= times[-1]:
        end_t = start_t + window_s
        mask = (times >= start_t) & (times < end_t)
        if not mask.any():
            start_t += stride_s
            continue

        win = coord_array[mask]
        win_lbls = frame_labels[mask]

        # 基本統計量
        mean, std = win.mean(axis=0), win.std(axis=0)
        mn, mx = win.min(axis=0), win.max(axis=0)

        # 移動距離
        reshaped = win.reshape(len(win), -1, 2)
        diffs = np.linalg.norm(np.diff(reshaped, axis=0), axis=2)
        dist_sum = diffs.sum(axis=0)

        # 部位間距離の統計（例: keypoint0とkeypoint1）
        k0 = reshaped[:,0,:]; k1 = reshaped[:,1,:]
        d01 = np.linalg.norm(k0 - k1, axis=1)
        d01_mean, d01_std = d01.mean(), d01.std()

        feat = np.concatenate([mean, std, mn, mx, dist_sum, [d01_mean, d01_std]])
        features.append(feat)

        # 窓ラベル: majority rule
        vals, counts = np.unique(win_lbls, return_counts=True)
        lbls.append(vals[np.argmax(counts)])

        start_t += stride_s

    return np.vstack(features), np.array(lbls)

# ===============================
# 4. 動画ごとに処理して統合
# ===============================
window_list = [0.25, 0.5, 1.0, 2.0]  # 秒単位ウィンドウ
results = []

for win in window_list:
    all_X, all_y, all_groups = [], [], []

    for vid in coords["video_id"].unique():
        coords_vid = coords[coords["video_id"]==vid].copy()
        fps = coords_vid["frames_per_sec"].iloc[0]
        frame_labels = np.array(["none"]*len(coords_vid))

        # ラベルをフレーム単位で展開
        anno_vid = anno[anno["video_id"]==vid]
        for _, row in anno_vid.iterrows():
            start_frame = int(row["start_frame"])
            end_frame   = int(row["end_frame"])
            frame_labels[start_frame:end_frame+1] = row["behavior"]

        coord_array = coords_vid.drop(columns=["frame","frames_per_sec","video_id"]).values

        X, y = window_features_seconds(coord_array, frame_labels, fps,
                                       window_s=win, stride_s=win*0.5)

        mask = y != "none"
        all_X.append(X[mask])
        all_y.append(y[mask])
        all_groups.append(np.full(sum(mask), vid))

    X_full = np.vstack(all_X)
    y_full = np.concatenate(all_y)
    groups_full = np.concatenate(all_groups)

    # ===============================
    # 5. LightGBM + GroupKFold CV
    # ===============================
    clf = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, num_leaves=31)
    cv = GroupKFold(n_splits=5)
    scores = cross_val_score(clf, X_full, y_full, cv=cv, groups=groups_full,
                             scoring=make_scorer(f1_score, average="macro"))
    print(f"Window={win:.2f}s | F1_macro={scores.mean():.4f} ± {scores.std():.4f}")
    results.append((win, scores.mean(), scores.std()))
```

---

### ✅ ポイントまとめ

1. **秒単位でウィンドウ化**して FPS の違いを吸収
2. **動画ごとに処理 → GroupKFold** で動画単位の CV
3. **ラベルは majority rule**。必要なら短イベント用に中心方式に変更可
4. 複数ウィンドウ長で比較 → 最適な秒単位ウィンドウを選定

---

💡 次のステップとしては：

* 窓の長さを 0.25〜2 秒で比較して F1 が最大のものを選ぶ
* 最良ウィンドウで LSTM / 1D-CNN に移行して時系列依存をモデル化

---

もし希望なら、このスクリプトに **短イベントを優先する「中心方式ラベル付け」バージョン** も追加して作れます。
作りますか？





Q: お願い

A: 了解 👍
では、今の「秒単位ウィンドウ化 + FPS補正 + GroupKFold CV」スクリプトに、
**短イベントを優先できる「中心方式ラベル付け」** を追加した完全版を示します。

---

## 🧩 改良版：中心方式ラベル付け対応版

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupKFold, cross_val_score
from sklearn.metrics import f1_score, make_scorer
import lightgbm as lgb

# ===============================
# 1. データ読み込み
# ===============================
coords = pd.read_csv("train.csv")           # 各フレームの座標: frame, frames_per_sec, x1,y1,x2,y2,...
anno   = pd.read_csv("train_annotation.csv") # start_frame, end_frame, behavior, video_id

# ===============================
# 2. 秒単位に変換
# ===============================
anno["start_sec"] = anno["start_frame"] / anno["frames_per_sec"]
anno["end_sec"]   = anno["end_frame"]   / anno["frames_per_sec"]
coords["time_sec"] = coords["frame"] / coords["frames_per_sec"]

# ===============================
# 3. 秒単位ウィンドウ特徴量抽出関数
# ===============================
def window_features_seconds(coord_array, frame_labels, fps, window_s=1.0, stride_s=None, label_mode="majority"):
    """
    label_mode: 'majority' または 'center'
      - 'majority': ウィンドウ内で最も多いラベル
      - 'center'  : ウィンドウ中央のフレームのラベル（短イベント優先）
    """
    if stride_s is None:
        stride_s = window_s * 0.5

    times = np.arange(len(coord_array)) / fps
    n_frames, dim = coord_array.shape

    features, lbls = [], []
    start_t = 0.0
    while start_t + window_s <= times[-1]:
        end_t = start_t + window_s
        mask = (times >= start_t) & (times < end_t)
        if not mask.any():
            start_t += stride_s
            continue

        win = coord_array[mask]
        win_lbls = frame_labels[mask]

        # ---- 特徴量 ----
        mean, std = win.mean(axis=0), win.std(axis=0)
        mn, mx = win.min(axis=0), win.max(axis=0)

        reshaped = win.reshape(len(win), -1, 2)
        diffs = np.linalg.norm(np.diff(reshaped, axis=0), axis=2)
        dist_sum = diffs.sum(axis=0)

        k0 = reshaped[:,0,:]; k1 = reshaped[:,1,:]
        d01 = np.linalg.norm(k0 - k1, axis=1)
        d01_mean, d01_std = d01.mean(), d01.std()

        feat = np.concatenate([mean, std, mn, mx, dist_sum, [d01_mean, d01_std]])
        features.append(feat)

        # ---- ラベル決定 ----
        if label_mode == "majority":
            vals, counts = np.unique(win_lbls, return_counts=True)
            lbls.append(vals[np.argmax(counts)])
        elif label_mode == "center":
            mid_t = start_t + window_s / 2
            mid_frame = int(mid_t * fps)
            if mid_frame < len(frame_labels):
                lbls.append(frame_labels[mid_frame])
            else:
                lbls.append("none")

        start_t += stride_s

    return np.vstack(features), np.array(lbls)

# ===============================
# 4. 動画ごとに処理して統合
# ===============================
window_list = [0.25, 0.5, 1.0, 2.0]  # 秒単位ウィンドウ
label_mode = "center"  # ← 短イベント優先
results = []

for win in window_list:
    all_X, all_y, all_groups = [], [], []

    for vid in coords["video_id"].unique():
        coords_vid = coords[coords["video_id"]==vid].copy()
        fps = coords_vid["frames_per_sec"].iloc[0]
        frame_labels = np.array(["none"]*len(coords_vid))

        anno_vid = anno[anno["video_id"]==vid]
        for _, row in anno_vid.iterrows():
            start_frame = int(row["start_frame"])
            end_frame   = int(row["end_frame"])
            frame_labels[start_frame:end_frame+1] = row["behavior"]

        coord_array = coords_vid.drop(columns=["frame","frames_per_sec","video_id"]).values

        X, y = window_features_seconds(coord_array, frame_labels, fps,
                                       window_s=win, stride_s=win*0.5,
                                       label_mode=label_mode)

        mask = y != "none"
        all_X.append(X[mask])
        all_y.append(y[mask])
        all_groups.append(np.full(sum(mask), vid))

    X_full = np.vstack(all_X)
    y_full = np.concatenate(all_y)
    groups_full = np.concatenate(all_groups)

    # ===============================
    # 5. LightGBM + GroupKFold CV
    # ===============================
    clf = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, num_leaves=31)
    cv = GroupKFold(n_splits=5)
    scores = cross_val_score(clf, X_full, y_full, cv=cv, groups=groups_full,
                             scoring=make_scorer(f1_score, average="macro"))
    print(f"Window={win:.2f}s | LabelMode={label_mode} | F1_macro={scores.mean():.4f} ± {scores.std():.4f}")
    results.append((win, scores.mean(), scores.std()))
```

---

### 🧠 このバージョンのポイント

| 項目                      | 説明                                           |
| ----------------------- | -------------------------------------------- |
| `label_mode='center'`   | ウィンドウ中央のフレームのラベルを採用。短時間イベントが優先される（例：一瞬の攻撃など） |
| `label_mode='majority'` | 長時間イベントが優勢な場合はこちら（安定動作の検出向き）                 |
| 複数ウィンドウ長（0.25〜2秒）       | 行動時間スケールの探索が簡単にできる                           |
| GroupKFold              | 動画間リークを防ぐ                                    |
| LightGBM                | 大量データでも高速CV可能                                |

---

### 🔍 使い方のアドバイス

1. まず `"center"` モードで短イベント（例：attack, avoidなど）を評価
2. `"majority"` モードで安定動作（grooming, huddlingなど）を評価
3. 両者を比較して、**動作ごとに最適なウィンドウ長 + ラベル方式** を決める

---

希望があれば次に、
✅ この特徴量出力を `.npy` や `.parquet` にキャッシュして高速化
✅ 各行動ごとに最適ウィンドウ長を自動探索するスクリプト

も作成できます。
どちらを次にやりたいですか？





Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 




Q: 

A: 

